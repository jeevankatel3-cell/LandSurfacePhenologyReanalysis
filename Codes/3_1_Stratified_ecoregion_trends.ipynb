{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9b57e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trend analysis for each ecoregion of Nepal based on different SUs\n",
    "#Input: 1. Trend Raster (all fitted and significant only)\n",
    "#        2. DEM Raster (elevation, slope (degrees), aspect (degrees))\n",
    "\n",
    "# Workflow steps : 0. SUs stratified by ecoregions > elevation > aspect > slope\n",
    "#                 1. no of trend fitted pixels for each SU (>10 for valid)\n",
    "#                 2. percentage of significant pixels for each SU (> %5 for valid)\n",
    "#                 3. no of pixels with postive trend and negative trend\n",
    "#                 4. Trend Assymetry Ratio: #n/#p (> 2 or <0.5 for valid)\n",
    "#                 5. SUs that pass all criteria \n",
    "#                 6. Ecoregion trend (areal% and slope in both direction) derived from valid SUs\n",
    "\n",
    "#starting with a simple code only considering sos  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c257ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECO_ID     ECO_NAME\n",
    "# 81003     Eastern Himalayan alpine shrub and meadows\n",
    "# 40115     Himalayan subtropical broadleaf forests\n",
    "# 40301     Himalayan subtropical pine forests\n",
    "# 40403     Western Himalayan broadleaf forests\n",
    "# -9999     Rock and Ice\n",
    "# 40401     Eastern Himalayan broadleaf forests\n",
    "# 40166     Upper Gangetic Plains moist deciduous forests - assign value null at last for all variables\n",
    "# 81021     Western Himalayan alpine shrub and Meadows\n",
    "# 40701     Terai-Duar savanna and grasslands\n",
    "# 40501     Eastern Himalayan subalpine conifer forests\n",
    "# 40502     Western Himalayan subalpine conifer forests\n",
    "# 40120     Lower Gangetic Plains moist deciduous forests - assign value null at last for all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "26e64cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries and data\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr \n",
    "import rasterio as rio  \n",
    "import rioxarray as rxr \n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "ecoregion_rxr = rxr.open_rasterio(r\"..\\Data\\Ecoregion_raster\\ecoregions_raster.tif\")\n",
    "\n",
    "elev_rxr = rxr.open_rasterio(r\"..\\Data\\DEM_Rasters\\elevation.tif\")\n",
    "aspect_rxr = rxr.open_rasterio(r\"..\\Data\\DEM_Rasters\\aspect.tif\")\n",
    "slope_rxr = rxr.open_rasterio(r\"..\\Data\\DEM_Rasters\\slope.tif\")\n",
    "\n",
    "roi_gdf = gpd.read_file(r\"../Data/roi_nepal/nepal_actual_roi.shp\")\n",
    "roi = roi_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "output_dir = r\"../Data/Processed/Ecoregion_trends/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "711d0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess dem rasters, actual values to classes\n",
    "# divide elevation, slope, aspect into classes\n",
    "\n",
    "# Elevation classes: <1000: 1, 1000-2000: 2, 2000-3000: 3, 3000-4000: 4, >4000: 5\n",
    "elev_class = xr.where(elev_rxr < 1000, 1, \n",
    "                      xr.where(elev_rxr < 2000, 2,\n",
    "                               xr.where(elev_rxr < 3000, 3,\n",
    "                                        xr.where(elev_rxr < 4000, 4, 5))))\n",
    "\n",
    "# Slope classes: 0-2: 1, 2-15: 2, 15-30: 3, >30: 4              Reference: FAO(2006)\n",
    "slope_class = xr.where(slope_rxr < 2, 1,\n",
    "                       xr.where(slope_rxr < 15, 2,\n",
    "                                xr.where(slope_rxr < 30, 3, 4)))\n",
    "\n",
    "# Aspect classes: Northern (270-360, 0-90): 1, Southern (90-270): 2\n",
    "aspect_class = xr.where((aspect_rxr >= 0) & (aspect_rxr <= 90), 1,\n",
    "                        xr.where((aspect_rxr >= 270) & (aspect_rxr <= 360), 1,\n",
    "                                 xr.where((aspect_rxr > 90) & (aspect_rxr < 270), 2, np.nan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9625b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A S U S\\AppData\\Local\\Temp\\ipykernel_19744\\639412932.py:34: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n",
      "  stacked_xr = xr.merge(aligned_rasters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Statistics for SOS\n",
      "==================================================\n",
      "total_pixels: 2569062\n",
      "total trend fitted pixels: 2047814\n",
      "total significant pixels: 159978\n",
      "trend fitted / total  %: 79.71\n",
      "significant / trend fitted  %: 7.81\n",
      "significant / total  %: 6.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A S U S\\AppData\\Local\\Temp\\ipykernel_19744\\639412932.py:34: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n",
      "  stacked_xr = xr.merge(aligned_rasters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Statistics for EOS\n",
      "==================================================\n",
      "total_pixels: 2569062\n",
      "total trend fitted pixels: 2048897\n",
      "total significant pixels: 155358\n",
      "trend fitted / total  %: 79.75\n",
      "significant / trend fitted  %: 7.58\n",
      "significant / total  %: 6.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A S U S\\AppData\\Local\\Temp\\ipykernel_19744\\639412932.py:34: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n",
      "  stacked_xr = xr.merge(aligned_rasters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Statistics for LOS\n",
      "==================================================\n",
      "total_pixels: 2569062\n",
      "total trend fitted pixels: 2048897\n",
      "total significant pixels: 191566\n",
      "trend fitted / total  %: 79.75\n",
      "significant / trend fitted  %: 9.35\n",
      "significant / total  %: 7.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A S U S\\AppData\\Local\\Temp\\ipykernel_19744\\639412932.py:34: FutureWarning: In a future version of xarray the default value for compat will change from compat='no_conflicts' to compat='override'. This is likely to lead to different results when combining overlapping variables with the same name. To opt in to new defaults and get rid of these warnings now use `set_options(use_new_combine_kwarg_defaults=True) or set compat explicitly.\n",
      "  stacked_xr = xr.merge(aligned_rasters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Statistics for POS\n",
      "==================================================\n",
      "total_pixels: 2569062\n",
      "total trend fitted pixels: 2048897\n",
      "total significant pixels: 514601\n",
      "trend fitted / total  %: 79.75\n",
      "significant / trend fitted  %: 25.12\n",
      "significant / total  %: 20.03\n"
     ]
    }
   ],
   "source": [
    "lsp_metrics = ['sos', 'eos', 'los', 'pos']\n",
    "\n",
    "# Initialize list to store pixel statistics for each metric\n",
    "pixel_stats_list = []\n",
    "\n",
    "for metric in lsp_metrics:\n",
    "    trend_rxr = rxr.open_rasterio(r\"..\\Data\\Trend_Rasters\\mod_\"+metric+\"_mk_raw.tif\")\n",
    "    sig_trend_rxr = rxr.open_rasterio(r\"..\\Data\\Trend_Rasters\\mod_\"+metric+\"_mk_significant.tif\")\n",
    "    final_outdir = os.path.join(output_dir, f\"{metric}\")\n",
    "    os.makedirs(final_outdir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    #stack rasters to create a pd dataframe\n",
    "    ref = sig_trend_rxr.rio.clip(roi.geometry, roi.crs, drop=True)\n",
    "\n",
    "    raster_dict = {\n",
    "        \"ecoregion\": ecoregion_rxr,\n",
    "        \"elevation\": elev_class,\n",
    "        \"slope\": slope_class,\n",
    "        \"aspect\": aspect_class,\n",
    "        \"trend\": trend_rxr,\n",
    "        \"sig_trend\": sig_trend_rxr\n",
    "    }\n",
    "\n",
    "    aligned_rasters = []\n",
    "\n",
    "    for name, raster in raster_dict.items():\n",
    "        raster = raster.rio.write_crs(\"EPSG: 4326\")\n",
    "        reproj = raster.rio.reproject_match(ref)\n",
    "        reproj.name = name\n",
    "        reproj = reproj.squeeze('band', drop = True)\n",
    "        aligned_rasters.append(reproj)\n",
    "\n",
    "    stacked_xr = xr.merge(aligned_rasters)\n",
    "    stacked_df = stacked_xr.to_dataframe().reset_index()\n",
    "    \n",
    "    #filter for select ecoregions\n",
    "    select_ecoregions = [81003, 40115, 40301, 40403, 40401, 40166, 81021, 40701, 40501, 40502, 40120]\n",
    "    stacked_df = stacked_df[stacked_df['ecoregion'].isin(select_ecoregions)]\n",
    "    stacked_df1 = stacked_df[~((stacked_df['trend'] == -999) )]\n",
    "    stacked_df2 = stacked_df[~((stacked_df['trend'] == -999) | (stacked_df['sig_trend'] == -999))]\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_pixels = stacked_df.shape[0]\n",
    "    total_trend_fitted = stacked_df1.shape[0]\n",
    "    total_significant = stacked_df2.shape[0]\n",
    "    \n",
    "    trend_fitted_pct = (total_trend_fitted / total_pixels) * 100\n",
    "    sig_from_fitted_pct = (total_significant / total_trend_fitted) * 100\n",
    "    sig_from_total_pct = (total_significant / total_pixels) * 100\n",
    "    \n",
    "    # Print statistics (original behavior)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Statistics for {metric.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(\"total_pixels:\", total_pixels)\n",
    "    print(\"total trend fitted pixels:\", total_trend_fitted)\n",
    "    print(\"total significant pixels:\", total_significant)\n",
    "    print(f\"trend fitted / total  %: {trend_fitted_pct:.2f}\")\n",
    "    print(f\"significant / trend fitted  %: {sig_from_fitted_pct:.2f}\")\n",
    "    print(f\"significant / total  %: {sig_from_total_pct:.2f}\")\n",
    "    \n",
    "    # Store statistics for CSV\n",
    "    pixel_stats_list.append({\n",
    "        'lsp_metric': metric,\n",
    "        'total_pixels': total_pixels,\n",
    "        'trend_fitted_pixels': total_trend_fitted,\n",
    "        'significant_pixels': total_significant,\n",
    "        'trend_fitted_pct': round(trend_fitted_pct, 2),\n",
    "        'sig_from_fitted_pct': round(sig_from_fitted_pct, 2),\n",
    "        'sig_from_total_pct': round(sig_from_total_pct, 2)\n",
    "    })\n",
    "\n",
    "    stacked_df['trend'] = stacked_df['trend'].fillna(-999)\n",
    "\n",
    "    su_stats = stacked_df.groupby(['ecoregion', 'elevation', 'slope', 'aspect']).agg(\n",
    "        \n",
    "        # 1. Trend Pixels\n",
    "        n_trend_unfitted_count=('trend', lambda x: (x == -999).sum()),\n",
    "        n_trend_fitted_count=('trend', lambda x: (x != -999).sum()),\n",
    "        \n",
    "        # 2. Significance Pixels\n",
    "        n_insig_trend_count=('sig_trend', lambda x: (x == -999).sum()),\n",
    "        n_sig_trend_count=('sig_trend', lambda x: ((x != -999) & (~x.isna())).sum()),\n",
    "        \n",
    "        # Positive and negative trends (for significant pixels only)\n",
    "        positive_sig_trend_count=('sig_trend', lambda x: ((x > 0) & (x != -999)).sum()),\n",
    "        negative_sig_trend_count=('sig_trend', lambda x: ((x < 0) & (x != -999)).sum()),\n",
    "        \n",
    "        # Mean values (excluding -999 and NaN)\n",
    "        positive_sig_trend_mean=('sig_trend', lambda x: x[(x > 0) & (x != -999)].mean() if len(x[(x > 0) & (x != -999)]) > 0 else np.nan),\n",
    "        negative_sig_trend_mean=('sig_trend', lambda x: x[(x < 0) & (x != -999)].mean() if len(x[(x < 0) & (x != -999)]) > 0 else np.nan),\n",
    "        all_sig_trend_mean=('sig_trend', lambda x: x[(x != -999) & (~x.isna())].mean())\n",
    "    ).reset_index()\n",
    "\n",
    "    # Add derived metrics based on your workflow\n",
    "    su_stats['percent_sig_pixels'] = (su_stats['n_sig_trend_count'] / su_stats['n_trend_fitted_count']) * 100\n",
    "    su_stats['trend_asymmetry_ratio'] = su_stats['positive_sig_trend_count'] / su_stats['negative_sig_trend_count']\n",
    "\n",
    "\n",
    "    #based on the set criteria we divide SUs into either 'valid' or 'invalid' category in lsp_change\n",
    "    su_stats['lsp_change'] = np.where(\n",
    "        (su_stats['n_trend_fitted_count'] > 10) &  # At least 10 fitted pixels\n",
    "        (su_stats['percent_sig_pixels'] > 5) &      # At least 5% significant\n",
    "        ((su_stats['trend_asymmetry_ratio'] > 2) | (su_stats['trend_asymmetry_ratio'] < 0.5)),  # Strong asymmetry\n",
    "        1,                  #means yes or valid\n",
    "        0                   #means no or invalid\n",
    "    )\n",
    "\n",
    "    su_stats.to_csv(os.path.join(final_outdir, \"All_SU_Stats_\"+metric+\".csv\"), index=False)\n",
    "\n",
    "    #from this step we drop all SUs with invalid lsp_change\n",
    "    # all % are calculated based on the ecoregions area (total pixels of that ecoregions)\n",
    "\n",
    "    ecr_count = su_stats.groupby('ecoregion')[['n_trend_fitted_count', 'n_trend_unfitted_count']].sum().reset_index()\n",
    "    ecr_count['total_pixels'] = ecr_count['n_trend_fitted_count'] + ecr_count['n_trend_unfitted_count']\n",
    "    ecr_count = ecr_count.drop(columns=['n_trend_fitted_count', 'n_trend_unfitted_count'])\n",
    "\n",
    "    su_filtered = (su_stats[su_stats['lsp_change'] == 1]).drop(columns=['lsp_change'])\n",
    "    ecr_stats = su_filtered.groupby('ecoregion').agg(\n",
    "        n_trend_fitted_count = ('n_trend_fitted_count', 'sum'),\n",
    "        n_sig_trend_count=('n_sig_trend_count', 'sum'),\n",
    "        positive_sig_trend_count=('positive_sig_trend_count', 'sum'),\n",
    "        negative_sig_trend_count=('negative_sig_trend_count', 'sum'),\n",
    "        positive_sig_trend_mean=('positive_sig_trend_mean', 'mean'),\n",
    "        negative_sig_trend_mean=('negative_sig_trend_mean', 'mean'),\n",
    "        all_sig_trend_mean=('all_sig_trend_mean', 'mean')\n",
    "    )\n",
    "\n",
    "    ecr_stats = pd.merge(ecr_stats, ecr_count, on='ecoregion', how = 'inner')    \n",
    "\n",
    "    ecr_stats['percent_trend_fit_px'] = (ecr_stats['n_trend_fitted_count'] / ecr_stats['total_pixels']) * 100\n",
    "    ecr_stats['percent_significant_valid_px'] = (ecr_stats['n_sig_trend_count'] / ecr_stats['total_pixels']) * 100\n",
    "    ecr_stats['percent_positive_valid_px'] = (ecr_stats['positive_sig_trend_count'] / ecr_stats['total_pixels']) * 100\n",
    "    ecr_stats['percent_negative_valid_px'] = (ecr_stats['negative_sig_trend_count'] / ecr_stats['total_pixels']) * 100\n",
    "    ecr_stats['trend_asymmetry_ratio'] = ecr_stats['positive_sig_trend_count'] / ecr_stats['negative_sig_trend_count']\n",
    "\n",
    "    #calculate net area % and net trend mean for ecoregions with notably asymmetric trend\n",
    "    ecr_stats['net_area_percent'] = np.where(ecr_stats['trend_asymmetry_ratio'] > 2,\n",
    "        ecr_stats['percent_positive_valid_px'] - ecr_stats['percent_negative_valid_px'],\n",
    "        np.where(ecr_stats['trend_asymmetry_ratio'] < 0.5,\n",
    "            ecr_stats['percent_positive_valid_px'] - ecr_stats['percent_negative_valid_px'],\n",
    "            0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ecr_stats = ecr_stats[['ecoregion','percent_trend_fit_px','percent_significant_valid_px','percent_positive_valid_px','percent_negative_valid_px','trend_asymmetry_ratio',\n",
    "                'positive_sig_trend_mean','negative_sig_trend_mean','all_sig_trend_mean','net_area_percent']]\n",
    "    ecr_stats.to_csv(os.path.join(final_outdir, \"ecoregion_stats.csv\"), index=False)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "pixel_stats_df = pd.DataFrame(pixel_stats_list)\n",
    "pixel_stats_df.to_csv(os.path.join(output_dir, \"pixel_statistics_summary.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d5052a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Topographic Driver Analysis - Which factors control LSP change direction?\n",
    "# # For each ecoregion, test if Elevation, Slope, or Aspect significantly influence trend asymmetry\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.stats import spearmanr, mannwhitneyu\n",
    "\n",
    "# # Step 1: Create normalized asymmetry index (avoids division by zero)\n",
    "# # Range: -1 (all positive trends) to +1 (all negative trends)\n",
    "# su_stats['asymmetry_index'] = (\n",
    "#     (su_stats['positive_sig_trend_count'] - su_stats['negative_sig_trend_count']) /\n",
    "#     (su_stats['positive_sig_trend_count'] + su_stats['negative_sig_trend_count'])\n",
    "# )\n",
    "\n",
    "# # Step 2: Filter for valid SUs only\n",
    "# valid_sus = su_stats[su_stats['lsp_change'] == 1].dropna(subset=['asymmetry_index'])\n",
    "\n",
    "# # Step 3: Test each ecoregion\n",
    "# driver_results = []\n",
    "\n",
    "# for eco_id, group in valid_sus.groupby('ecoregion'):\n",
    "#     if len(group) < 5:  # Skip if too few SUs\n",
    "#         continue\n",
    "    \n",
    "#     # --- ELEVATION: Spearman correlation ---\n",
    "#     rho_elev, p_elev = spearmanr(group['elevation'], group['asymmetry_index'])\n",
    "#     is_elev_driver = (p_elev < 0.05) and (abs(rho_elev) > 0.1)\n",
    "    \n",
    "#     # --- SLOPE: Spearman correlation ---\n",
    "#     rho_slope, p_slope = spearmanr(group['slope'], group['asymmetry_index'])\n",
    "#     is_slope_driver = (p_slope < 0.05) and (abs(rho_slope) > 0.1)\n",
    "    \n",
    "#     # --- ASPECT: Mann-Whitney with directional effect size ---\n",
    "#     north = group[group['aspect'] == 1]['asymmetry_index']\n",
    "#     south = group[group['aspect'] == 2]['asymmetry_index']\n",
    "    \n",
    "#     if len(north) > 5 and len(south) > 5:\n",
    "#         u_stat, p_aspect = mannwhitneyu(north, south)\n",
    "        \n",
    "#         # Calculate effect size r = |Z| / sqrt(N)\n",
    "#         n1, n2 = len(north), len(south)\n",
    "#         mu = n1 * n2 / 2\n",
    "#         sigma = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)\n",
    "#         z_score = (u_stat - mu) / sigma\n",
    "#         r_magnitude = abs(z_score) / np.sqrt(n1 + n2)\n",
    "        \n",
    "#         # Assign direction: + if North has higher asymmetry, - if South higher\n",
    "#         r_aspect = r_magnitude if north.median() > south.median() else -r_magnitude\n",
    "        \n",
    "#         is_aspect_driver = (p_aspect < 0.05) and (abs(r_aspect) > 0.1)\n",
    "#     else:\n",
    "#         r_aspect, p_aspect, is_aspect_driver = 0, 1.0, False\n",
    "    \n",
    "#     # Store results\n",
    "#     driver_results.append({\n",
    "#         'ecoregion': eco_id,\n",
    "#         'elev_r': round(rho_elev, 3),\n",
    "#         'slope_r': round(rho_slope, 3),\n",
    "#         'aspect_r': round(r_aspect, 3),\n",
    "#         'elev_p': round(p_elev, 4),\n",
    "#         'slope_p': round(p_slope, 4),\n",
    "#         'aspect_p': round(p_aspect, 4),\n",
    "#         'is_elev_driver': 'YES' if is_elev_driver else 'NO',\n",
    "#         'is_slope_driver': 'YES' if is_slope_driver else 'NO',\n",
    "#         'is_aspect_driver': 'YES' if is_aspect_driver else 'NO'\n",
    "#     })\n",
    "\n",
    "# # Step 4: Create results table\n",
    "# drivers_df = pd.DataFrame(driver_results)\n",
    "# print(\"\\n=== Topographic Drivers of LSP Trend Direction ===\")\n",
    "# print(\"Criteria: p < 0.05 AND |r| > 0.1\")\n",
    "# print(\"\\nInterpretation:\")\n",
    "# print(\"  + value = Higher class/North aspect → more positive trends\")\n",
    "# print(\"  - value = Higher class/South aspect → more negative trends\\n\")\n",
    "# display(drivers_df)\n",
    "\n",
    "# # Optional: Save results\n",
    "# #drivers_df.to_csv(\"Topographic_Drivers.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
